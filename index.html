<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Grasping Point Detection</title>
  <meta name="description" content="Project page for Grasping Point Detection with local images and video demos." />
  <meta property="og:title" content="Grasping Point Detection" />
  <meta property="og:description" content="Dataset preview, grasping direction visualization, and a cluttered-scene demo video." />
  <meta property="og:image" content="static/images/dataset.png" />
  <meta property="og:type" content="website" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;800&display=swap" rel="stylesheet">
  <style>
    :root{--wrap:1100px;--bg:#0c0f16;--card:#11182a;--border:#1c2c4d;--text:#e9eef8;--muted:#a8b4c9;}
    *{box-sizing:border-box}
    html,body{margin:0;background:var(--bg);color:var(--text);font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;line-height:1.6}
    a{color:#8cc6ff;text-decoration:none} a:hover{text-decoration:underline}
    .wrap{max-width:var(--wrap);margin:0 auto;padding:0 20px}
    header{padding:72px 0;background:linear-gradient(180deg,#0e1422,#10182b)}
    h1{font-size:clamp(28px,6vw,56px);margin:0 0 8px}
    p.lead{color:var(--muted);font-size:clamp(16px,2.6vw,20px);margin:0}
    .authors{margin-top:10px;color:#dfe7ff}
    .btns{display:flex;flex-wrap:wrap;gap:10px;margin-top:16px}
    .btn{display:inline-flex;align-items:center;gap:8px;padding:10px 14px;border-radius:999px;background:#172746;border:1px solid var(--border);color:#e6eefb}
    section{padding:32px 0}
    .grid{display:grid;gap:18px}
    @media(min-width:900px){.grid.cols-2{grid-template-columns:1fr 1fr}.grid.cols-3{grid-template-columns:repeat(3,1fr)}}
    .card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:18px}
    img,video{max-width:100%;height:auto;border-radius:12px}
    pre.code{background:#0a1020;padding:14px;border-radius:12px;overflow:auto;border:1px solid var(--border)}
    footer{border-top:1px solid var(--border);color:#97a6c3;padding:28px 0}
  </style>
</head>
<body>
  <header>
    <div class="wrap">
      <h1>Grasping Point Detection</h1>
      <p class="lead">Local, self‚Äëhosted demo page with images and a cluttered‚Äëscene video. No external embeds.</p>
      <div class="authors">By <strong>Bobobs</strong></div>
      <div class="btns">
        <a class="btn" href="https://github.com/Bobobs/grasping-point-detection">üíª Code (GitHub)</a>
        <a class="btn" href="#media">üñºÔ∏è Media</a>
        <a class="btn" href="#method">üìò Overview</a>
      </div>
    </div>
  </header>

  <!-- Media section uses your actual files: dataset.png, grasping_direction.jpg, cluttered.mp4 -->
  <section id="media">
    <div class="wrap">
      <h2>Media</h2>
      <div class="grid cols-2">
        <div class="card">
          <h3>Dataset Preview</h3>
          <img src="static/images/dataset.png" alt="Dataset preview" />
          <p class="lead">A quick look at the dataset used for grasping point detection.</p>
        </div>
        <div class="card">
          <h3>Grasping Direction Visualization</h3>
          <img src="static/images/grasping_direction.jpg" alt="Grasping direction visualization" />
          <p class="lead">Illustration of the estimated grasping direction on sample data.</p>
        </div>
      </div>

      <div class="card" style="margin-top:18px">
        <h3>Cluttered Scene Demo (MP4)</h3>
        <video controls loop muted playsinline preload="metadata">
          <source src="static/videos/cluttered.mp4" type="video/mp4" />
          Your browser does not support the video tag.
        </video>
        <p class="lead">Short demonstration on a cluttered environment.</p>
      </div>
    </div>
  </section>

  <section id="method">
    <div class="wrap">
      <h2>Overview</h2>
      <p>
        In this work, we propose a new framework that
        leverages semantic segmentation of clothes to further detect
        optimal grasping points directly from RGB-D images. Our
        framework involves a novel neural network architecture that
        fuses self-attention and convolutional operations to extract potent
        RGB-D features, facilitating the semantic segmentation of cloth
        inner/outer edges and corners. The framework then couples a
        hierarchy of these semantic features with corresponding depth
        cues to identify the best grasping point candidates based on both
        geometric and depth information. To evaluate the performance
        of our framework, we introduce a novel RGB-D garment dataset
        containing 298 RGB-D images of 12 distinct textile objects
        annotated with corners and inner/outer edges. We benchmarked
        the performance of our model against the state-of-the-art models
        on both our own and the NYU Depth v2 datasets. Our model
        surpasses the baseline by 1.1% on NYU Depth V2 and 0.95% on
        our dataset. Our extensive experiments with a Kinova robotic
        arm showed that in cluttered scenes, our proposed method can
        achieve up to 27% higher grasping success than the baseline
        model. 
      </p>
    </div>
  </section>

  <footer>
    <div class="wrap">¬© <span id="year"></span> Bobobs ¬∑ Project page template adapted for local media.</div>
  </footer>

  <script>document.getElementById('year').textContent = new Date().getFullYear();</script>
</body>
</html>